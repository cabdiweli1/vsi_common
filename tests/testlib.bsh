#!/usr/bin/env false

#*# tests/testlib

#**
# ============
# Test Library
# ============
#
# .. default-domain:: bash
#
# .. file:: testlib.bsh
#
# Simple shell command language test library
#
# .. rubric:: Usage
#
# . testlib.bsh
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   Tests must follow the basic form:
#
#   source testlib.bsh
#
#   begin_test "the thing"
#   (
#        set -e
#        echo "hello"
#        false
#   )
#   end_test
#
#   When a test fails its stdout and stderr are shown.
#
# .. note::
#   Tests must 'set -e' within the subshell block or failed assertions will not cause the test to fail and the result may be misreported. While this is not required, most tests will have this on.
#
# .. rubric:: Bugs
#
# On darling: when debugging a unit test error, sometimes the printout is cut off, making it difficult to do "printf debugging." While the cause and scope of this is unknown, a work around that sometimes works is
#
# .. code-block:: bash
#
#     runtests 2>&1 | less -R
#
# :Copyright: Original version: (c) 2011-13 by Ryan Tomayko <http://tomayko.com>
#
#             License: MIT
# :Author: Ryan Tomayko
# :Modification History: Andy Neff
#
#              * Added :func:`begin_expected_fail_test`
#              * Added optional :func:`setup`/:func:`teardown` functions
#              * Removed PATH
#              * Added robodoc documentation
#              * Use pushd/popd for each test instead of cd
#              * Auto prepend filename to description
#              * Added custom PS4
#**

: ${VSI_COMMON_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.."; pwd)"}

set -e

if [ "${TESTLIB_SHOW_TIMING-0}" == "1" ] || [[ ${OSTYPE} = darwin* ]]; then
  . "${VSI_COMMON_DIR}/linux/time_tools.bsh"
fi
. "${VSI_COMMON_DIR}/linux/file_tools.bsh"
. "${VSI_COMMON_DIR}/linux/dir_tools.bsh"
. "${VSI_COMMON_DIR}/tests/test_colors.sh"
. "${VSI_COMMON_DIR}/linux/signal_tools.bsh"

# create a temporary work space
TRASHDIR="$(mktemp -d -t $(basename "$0")-$$.XXXXXXXX)"

# keep track of num tests and failures
tests=0
failures=0
expected_failures=0
required_failures=0
skipped=0

#**
# .. function:: setup
#
# Function run before the first test
#
# .. note::
#   A directory :envvar:`TRASHDIR` is created for setup, right before running :func:`setup` ().
#
#   Setup is not run if no tests are ever run
#**

#**
# .. envvar:: TRASHDIR
#
# Temporary directory where everything for the test file is stored
#
# Automatically generated and removed (unless :envvar:`TEST_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TESTDIR`
#
# :Author: Ryan Tomayko
#**

#**
# .. envvar:: TESTDIR
#
# Unique temporary directory for a single test (in :envvar:`TRASHDIR`)
#
# Automatically generated and removed (unless :envvar:`TEST_KEEP_TEMP_DIRS` is changed)
#
# .. seealso::
#   :envvar:`TRASHDIR`
#
# :Author: Ryan Tomayko
#**

#**
# .. function:: teardown
#
# Function run after the last test
#
# .. note::
#   Teardown is not run if no tests are ever run
#**

#**
# .. envvar:: TEST_KEEP_TEMP_DIRS
#
# Keep the trashdir/setup dir
#
# Debug flag to keep the temporary directories generated when testing. Set to ``1`` to keep directories.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_SHOW_TIMING
#
# Display test time after each test
#
# Debug flag to display time elapsed for each test. Set to ``1`` to enable.
#
# :Default: ``0``
#**

#**
# .. envvar:: TESTLIB_RUN_SINGLE_TEST
#
# Run a single test
#
# Instead of running all the tests in a test file, only the tests with a description exactly matching the value of :envvar:`TESTLIB_RUN_SINGLE_TEST` will be run. Useful for debugging a specific test/piece of code
#
# :Default: *unset*
#**

#**
# .. function:: atexit
#
# Function that runs at process exit
#
# .. rubric:: Usage
#
# Automatically called on exit by trap.
#
# Checks to see if teardown is defined, and calls it. teardown is typically a function, alias, or something that makes sense to call.
#
# :Author: Ryan Tomaydo
# :Modification History: Andy Neff
#
#             * Added :func:`setup` cleanup
#             * Added :func:`teardown`
#             * Added :envvar:`TEST_KEEP_TEMP_DIRS` flags
#**
atexit ()
{
  test_status=$?
  if [ -n "${test_description+set}" ]; then
    end_test $test_status
    echo "${TEST_BAD_COLOR}WARNING${TEST_RESET_COLOR}: end_test not added at end of last test." 1>&2
    declare -p BASH_SOURCE
  fi
  unset test_status

  if [ "${tests}" -ne 0 ] && [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  if [ "${tests}" -ne 0 ] && type -t teardown &> /dev/null && [ "$(command -v teardown)" == "teardown" ]; then
    teardown
  fi

  if [ "${TEST_KEEP_TEMP_DIRS-}" != "1" ]; then
    rm -rf "$TRASHDIR"
  fi

  local BOLD_COLOR
  if [ "${failures}" -eq 0 ]; then
    BOLD_COLOR="${TEST_BOLD_COLOR}"
  else
    BOLD_COLOR="${TEST_BAD_COLOR}"
  fi

  printf "%s summary: %d test ran, ${BOLD_COLOR}%d failures${TEST_RESET_COLOR}, %d expected failures, %d required failures, %d skipped\n" \
         "$0" "${tests}" "${failures}" "${expected_failures}" "${required_failures}" "${skipped}"

  if [ -d "${summary_log_dir-}" ]; then
    echo "${tests} ${failures} ${expected_failures} ${required_failures} ${skipped}" > "${summary_log_dir}/$(basename "$0")"
  fi

  if [ "${failures}" -gt 0 ]; then
    exit 1
  else
    exit 0
  fi
}
trap_chain "atexit" EXIT

if declare -p BASH_SOURCE &> /dev/null; then
  PS4=$'+${BASH_SOURCE[0]##*/}:${LINENO})\t'
else # Else sh probably
  # Not as accurate, but better than nothing
  PS4=$'+${0##*/}:${LINENO})\t'
fi

# Common code for begin tests
_begin_common_test ()
{
  local fd
  TESTDIR="${TRASHDIR}/test${tests}"
  mkdir -p "${TESTDIR}"
  pushd "$TESTDIR" &> /dev/null
  # This makes calling end_test between tests "optional", but highly recommended.
  # end_test does have to be called after the last test, especially if teardown
  # is defined after the last test
  if [ -n "${test_description+set}" ]; then
    end_test $test_status
    echo "${TEST_BAD_COLOR}WARNING${TEST_RESET_COLOR}: end_test not added at end of a test." 1>&2
    declare -p BASH_SOURCE
  fi
  unset test_status

  # Set flag defaults that could be overrideable in certain test types.
  # This needs to be after end_test call above in order to keep end_test
  # optional
  expected_failure=${_expected_failure-0}
  required_fail=${_required_fail-0}

  # Run setup if this is the first test
  if [ "${tests}" -eq 0 ] && type -t setup &> /dev/null && [ "$(command -v setup)" == "setup" ]; then
    setup
  fi

  tests=$(( tests + 1 ))
  local test_file_name="$(basename "$0")"
  test_file_name=${test_file_name%.*}
  test_file_name=${test_file_name#test-}
  test_description="$test_file_name - $1"

  if [ "${tracking_touched_files}" = "1" ]; then
    track_touched_file="$(mktemp -u)"
    ttouch "${track_touched_file}"
  fi

  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    _time_0=$(get_time_seconds)
  fi

  if [ "${TESTLIB_RUN_SINGLE_TEST+set}" = "set" ] && \
     [ "$1" != "${TESTLIB_RUN_SINGLE_TEST}" ]; then
    skip_next_test
  fi

  if [ "${BASH_VERSINFO[0]}" -gt "4" ] || [ "${BASH_VERSINFO[0]}" == "4" ] && [ "${BASH_VERSINFO[1]}" -ge "1" ]; then
    exec {stdout}>&1
    exec {stderr}>&2
  else
    find_open_fd stdout
    find_open_fd stderr
    eval "exec ${stdout}>&1"
    eval "exec ${stderr}>&2"
  fi

  out="${TESTDIR}/out"
  err="${TESTDIR}/err"
  exec 1>"$out" 2>"$err"

  # Allow the subshell to exit non-zero without exiting this process
  set -x +e
}

#**
# .. function:: begin_test
#
# Beginning of test demarcation
#
# .. rubric:: Usage
#
# Mark the beginning of a test. A subshell should immediately follow this statement.
#
# .. seealso::
#   :func:`end_test`
#
# :Author: Ryan Tomayko
#**
begin_test ()
{
  test_status=$? # Must be first command
  _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_expected_fail_test
#
# Beginning of expected fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is expected to fail
#**
begin_expected_fail_test()
{
  test_status=$? # Must be first command
  # Override _begin_common_test default
  _expected_failure=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: begin_required_fail_test
#
# Beginning of required fail test demarcation
#
# .. rubric:: Usage
#
# Define the beginning of a test that is required to fail
#**
begin_required_fail_test()
{
  test_status=$? # Must be first command
  # Override _begin_common_test default
  _required_fail=1 _begin_common_test ${@+"${@}"}
}

#**
# .. function:: setup_test
#
# Sets up the test
#
# Once inside the () subshell, typically set -eu needs to be run, then other things such as checking to see if a test should be skipped, etc. need to be done. This is all encapsulated into :func:`setup_test`. This is required; without it, :func:`end_test` will know you forgot to call this and fail.
#
# This is also the second part of creating a skippable test.
#
# You are free to change "set -eu" after :func:`setup_test`, should you wish.
#
# .. rubric:: Usage
#
# Place at the beginning of a test
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   skip_next_test
#   begin_test "Skipping test"
#   (
#     setup_test
#     #test code here
#   )
#
# .. seealso::
#   :func:`skip_next_test`
#**
setup_test()
{
  # Identify that setup_test was called
  touch "${TRASHDIR}/.setup_test"

  # Check to see if this test should be skipped
  if [ "${__testlib_skip_test-}" = "1" ]; then
    exit 0
  fi

  set -eu
}

#**
# .. function:: end_test
#
# End of a test demarcation
#
# .. rubric:: Usage
#
# Mark the end of a test. Must be the first command after the test group, or else the return value will not be captured successfully.
#
# .. seealso::
#   :func:`begin_test`
#
# :Author: Ryan Tomayko
#**
end_test ()
{
  test_status="${1:-$?}" # This MUST be the first line of this function
  set +x -e
  # Tested, this apparently works on bash 3.2
  exec 1>&${stdout} 2>&${stderr}
  popd &> /dev/null

  local time_e=''
  if [ "${TESTLIB_SHOW_TIMING-0}" = "1" ]; then
    time_e=$(awk "BEGIN {print \"\t\" $(get_time_seconds)-${_time_0}}")
  fi

  if [ ! -e "${TRASHDIR}/.setup_test" ]; then
    # This is a "no matter what, failure". No expected/required failure work
    # around for this
    printf "%-80s ${TEST_BAD_COLOR}SETUP FAILURE${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    failures=$(( failures + 1 ))
  elif [ "${__testlib_skip_test-}" = "1" ] && [ "${test_status}" -eq 0 ]; then
    printf "%-80s ${TEST_GOOD_COLOR}SKIPPED${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    skipped=$((skipped+1))
  elif [ "${required_fail}" -eq 1 ] && [ "$test_status" -ne 0 ]; then
    printf "%-80s ${TEST_GOOD_COLOR}FAIL REQUIRED${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    required_failures=$((required_failures+1))
  elif [ "${required_fail}" -eq 0 ] && [ "$test_status" -eq 0 ]; then
    printf "%-80s ${TEST_GOOD_COLOR}OK${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
  elif [ "${expected_failure}" -eq 1 ]; then
    printf "%-80s ${TEST_GOOD_COLOR}FAIL EXPECTED${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    expected_failures=$(( expected_failures + 1 ))
  else
    failures=$(( failures + 1 ))
    if [ "${required_fail}" -eq 1 ]; then
      printf "%-80s ${TEST_BAD_COLOR}SHOULD HAVE FAILED${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    else
      printf "%-80s ${TEST_BAD_COLOR}FAILED${TEST_RESET_COLOR}%s\n" "${test_description}" "${time_e}"
    fi
    (
      echo "-- stdout --"
      sed 's/^/    /' <"${TESTDIR}/out"
      echo "-- stderr --"
      grep -v -e $'^\+[^\t]*\tend_test' \
              -e $'^\+[^\t]*\tset +x -e' <"${TESTDIR}/err" |
        sed $'s|^[^+]| \t&|' |
        column -c1 -s $'\t' -t |
        sed 's/^/    /'
      echo "-- EOF $test_description --"
    ) 1>&2
  fi

  if [ "${tracking_touched_files-}" = "1" ]; then
    cleanup_touched_files
  fi

  unset test_description
  unset __testlib_skip_test

  rm "${TRASHDIR}/.setup_test" || :
}

#**
# .. function:: skip_next_test
#
# Function to indicate the next test should be skipped
#
# This is the first part of creating a skippable test, used in conjunction with :func:`setup_test`
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   For example, skip if docker command not found
#
#     command -v docker &> /dev/null && skip_next_test
#     begin_test "My test"
#     (
#       setup_test
#       [ "$(docker run -it --rm ubuntu:14.04 echo hi)" = "hi" ]
#     )
#
# .. seealso::
#   :func:`setup_test`
#
# .. note::
#   This must be done outside of the test, or else the skip variable will not be set and detected by :func:`end_test`
#**
skip_next_test()
{
  __testlib_skip_test=1
}

#**
# .. function:: not
#
# :Arguments: ``$1``... - Command and arguments
# :Output: Return value
#
#     * ``0`` - On non-zero return code evaluation
#     * ``1`` - On zero return code
#
# Returns true only when the command fails
#
# Since ``!`` is ignored by "set -e", use :func:`not` instead. This is just a helper to make unittests look nice and not need extra ifs everywhere
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   # No good, always passes, even if ! true
#   ! false
#
#   # good
#   not false
#   # equivalent to
#   if ! false; then
#     true
#   else
#     false
#   fi
#
# .. rubric:: Bugs
#
# Complex statements do not work, e.g. [, [[ and ((, etc...
#   For example, you should use
#
# .. code-block:: bash
#
#     [ ! -e /test ]
#
# |  instead of
#
# .. code-block:: bash
#
#     not [ -e /test ]
#
# |  In cases where this is not easily worked around, you can use
#
# .. code-block:: bash
#
#     not_s '[ -e /test ]'
#
# .. seealso::
#   :func:`not_s`
#**
not()
{
  local cmd="$1"
  shift 1
  if "${cmd}" ${@+"${@}"}; then
    return 1
  else
    return 0
  fi
}

# Testing this idea...
#**
# .. function:: not_s
#
# :Arguments: ``$1`` - Command/statement in a single string
# :Output: Return Value:
#
#             * ``0`` - On non-zero return code evaluation
#             * ``1`` - On zero return code
#
# Returns true only when the string version of command fails
#
# Since ``!`` is ignored by "set -e", use :func:`not` instead. This is just a helper to make unittests look nice and not need extra ifs everywhere.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   x=test
#   y=t.st
#   not2 '[[ $x =~ $y ]]' # <-- notice single quotes.
#
# While the single quotes aren't necessary, they handle the more complicated situations more easily.
#
# .. note::
#   Uses eval
#
# .. seealso::
#   :func:`not`
#
#**
not_s()
{
  eval "if ${1}; then return 1; else return 0; fi"
}

#**
# .. function:: track_touched_files
#
# Start tracking touched files
#
# After running :func:`track_touched_files`, any call to touch will cause that file to be added to the internal list (touched_files). Just prior to the teardown phase, all of these files will be automatically removed for your convenience.
#
# .. rubric:: Example
#
# .. code-block:: bash
#
#   setup()
#   {
#     track_touched_files
#   }
#   begin_test Testing
#   (
#     touch /tmp/hiya
#   )
#   end_test
#
# .. rubric:: Usage
#
# Should be called before the :func:`begin_test` block, not inside. Inside a () subshell block will not work. Setup is the logical place to put it.
#
# .. rubric:: Bugs
#
# Does not work in sh, only ``bash``. Uses array, and I didn't want to make this use a string instead.
#
# Not thread safe. Use a different file for each thread
#
# .. seealso::
#   :func:`cleanup_touched_files`
#**
track_touched_files()
{
  tracking_touched_files=1
}

#**
# .. function:: ttouch
#
# Touch function that should behave like the original touch command
#
# .. seealso::
#   :func:`track_touched_files`
#**
ttouch()
{
  local filename
  local end_options=0

  touch "${@}"

  # Skip all options
  while [ $# -gt 0 ]; do
    if [ "${end_options}" = "0" ] && [ "$1" = "--" ]; then
      end_options=1
      shift 1
    elif [ "${end_options}" = "0" ] && [ ${#1} -gt 0 ] && [ "${1:0:1}" = "-" ]; then
      shift 1
    else
      filename="$1"
      # force to be absolute path
      if [ "${filename:0:1}" != "/" ]; then
        filename="$(pwd)/$1"
      fi
      printf "${filename}\0" >> "${track_touched_file}"
      shift 1
    fi
  done
}

#**
# .. function:: cleanup_touched_files
#
# Delete all the touched files
#
# At the end of the last test, delete all the files in the array
#**
cleanup_touched_files()
{
  local touched_file
  local touched_files
  local line

  # This will normally get called an extra time at exit, so the existence of
  # the file acts as a check.
  if [ -f "${track_touched_file}" ]; then
    while IFS='' read -r -d '' touched_file 2>/dev/null || [ -n "$touched_file" ]; do
      if [ -e "${touched_file}" ] && [ ! -d "${touched_file}" ]; then
        \rm "${touched_file}"
      fi

      touched_file='' #Have to clear it, in case the timeout times out
    done < "${track_touched_file}"
  fi
}